{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Bigram model\n",
        "\n",
        "I am going to be building a simple bigram model (as introduction to then build a transformer) following `Andrej Karpathy` tutorial `Let's build GPT: from scratch, in code, spelled out`.\n",
        "\n",
        "The idea is to improve our knowledge of transformers and connect from the theory to the actual code.\n",
        "\n",
        "To train our transformer we will use a Shakespeare dataset and in theory the model after training will be able to generate sequences of text like Shapespeare."
      ],
      "metadata": {
        "id": "9n64qEvhi4Kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import packages"
      ],
      "metadata": {
        "id": "fMOzRnOln1qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "gaIzCpgMn31z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device agnostic code"
      ],
      "metadata": {
        "id": "GFpVSErF13KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SUL6D1qJ1499",
        "outputId": "84467dc3-710b-4f1f-b515-48b57babfa95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the Shakespeare Dataset\n",
        "\n",
        "1. We are going to download the `.txt` from the karpathy repo.\n",
        "2. We are going to read it and store it in our code variable."
      ],
      "metadata": {
        "id": "9FuPZvcckRnM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_WcS0o0ZhQD6",
        "outputId": "5c7463c5-bc30-4464-dedf-6c24dde2eaac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-03 12:16:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-12-03 12:16:06 (22.9 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file=\"/content/input.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(f\"Aprox length of dataset: {len(text)}\\n\")\n",
        "print(f\"First 100 characters of the dataset:\\n{text[:100]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJaaGVKDkaiw",
        "outputId": "613ce083-7243-4129-9bad-887874c98772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aprox length of dataset: 1115394\n",
            "\n",
            "First 100 characters of the dataset:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build vocabulary based on characters (Character-level)\n",
        "\n",
        "This will be the characters that the model can see or 'generate'."
      ],
      "metadata": {
        "id": "S7xeFuLTlPnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Characters\n",
        "characters = sorted(list(set(text)))\n",
        "# Vocabulary size\n",
        "vocab_size = len(characters)\n",
        "\n",
        "print(f\"Characters: {''.join(characters)}\")\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcWy53Hxkzoh",
        "outputId": "8c2e90ae-98b7-4743-8009-34e8ea414d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocab size: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character level lookup tables\n",
        "\n",
        "This is a simple tokenizer for our characters/vocab. We will have a loop up table to transform characters into tokens/ints and another to do the reverse process (transform tokens/ints to characters/text)."
      ],
      "metadata": {
        "id": "mGS25mNWl_tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We create the lookup tables (Character-base)\n",
        "string_to_id = { char:idx for idx, char in enumerate(characters)}\n",
        "id_to_string = { idx:char for idx, char in enumerate(characters)}\n",
        "\n",
        "# Create the functions to look words in the lookup tables\n",
        "encode = lambda sentence: [string_to_id[char] for char in sentence]\n",
        "decode = lambda id_list: \"\".join(id_to_string[id] for id in id_list)"
      ],
      "metadata": {
        "id": "qQYAtduClk2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing our our tokenizers\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQhRxWb4nfcA",
        "outputId": "0f194eca-b537-4e21-d6d6-c0291340a4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the entire text\n",
        "\n",
        "We are going to encode the whole dataset and transform it into a tensor"
      ],
      "metadata": {
        "id": "_tHpD_-Jns6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(f\"Data tensor shape: {data.shape, data.dtype}\")\n",
        "print(f\"First 100 encodes: {data[:100]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZCB0ykxnk0A",
        "outputId": "c2147db2-605c-4273-9484-c31eeb55f39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data tensor shape: (torch.Size([1115394]), torch.int64)\n",
            "First 100 encodes: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data between training & validation set\n",
        "\n",
        "This will allow us to train our model and later test it with data it hasn't seen yet. We are going to do it manually (90% for training)."
      ],
      "metadata": {
        "id": "6Aciycd2oRBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the index where we need to \"cut\"\n",
        "n = int(0.9 * len(data))\n",
        "# Training set\n",
        "train_data = data[:n]\n",
        "# Validation set\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"Training length: {len(train_data)}\")\n",
        "print(f\"Validation length: {len(val_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75xoHhzuoJNJ",
        "outputId": "991baed4-b501-4dc9-9763-1805a1a8eb1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training length: 1003854\n",
            "Validation length: 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context size (Block size)\n",
        "\n",
        "We want to sample parts/chunks of the text. The idea is that the model can see want should come next from one character up to block size."
      ],
      "metadata": {
        "id": "2Bkx7YYWo9zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size + 1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKi1yi57oy8y",
        "outputId": "977a13c3-b2ab-46c4-804b-e3155f404d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size + 1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "\n",
        "  print(f\"When input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbAY5yu6pCzx",
        "outputId": "b85cb32f-7070-4ffb-efa6-26dcfac58aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([18]) the target: 47\n",
            "When input is tensor([18, 47]) the target: 56\n",
            "When input is tensor([18, 47, 56]) the target: 57\n",
            "When input is tensor([18, 47, 56, 57]) the target: 58\n",
            "When input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "When input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "When input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generalize it to also work with batches"
      ],
      "metadata": {
        "id": "gejgweQfqPRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "batch_size = 4 # Amount of sequences processed in parallel\n",
        "block_size = 8 # Max context\n",
        "\n",
        "def get_batch(split: Literal[\"train\", \"validation\"]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Generate a small batch of data of inputs x and targets y.\n",
        "  It first generates 4 random locations in the dataset, and then extract the data for each of those indexes.\n",
        "\n",
        "  Args:\n",
        "    split (\"train\" | \"validation\"): What data split to use\n",
        "  Returns:\n",
        "    x -> Tensor\n",
        "    y -> Tensor\n",
        "  \"\"\"\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  # Sample random parts\n",
        "  ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "  # Get the actual data for that random sample\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "I1Uw18lGpkng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC2aUD4urosR",
        "outputId": "7b9ce364-3f5d-4c15-ed99-1f5652aeae5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple model (Bigram)\n",
        "\n",
        "We are going to be implementing a simpler model first and then move to the more complex one. We will start with a `Bigram Language Model`. Bigrams would only take the last idx when generating but as we are later going to build the transformer, which takes the whole sequence, it help us understand how the data flows and everything works together."
      ],
      "metadata": {
        "id": "R2ddh1WwsVrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  \"\"\"\n",
        "  Predicts the next token only using the current token\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size: int):\n",
        "    \"\"\"\n",
        "    Initializes the Bigram class\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"\n",
        "    Forward method for the BigramLanguageModel\n",
        "    \"\"\"\n",
        "    # (B,T,C) -> (Batch_size, Sequence length, vocab_size)\n",
        "    logits = self.token_embedding_table(idx)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # Transform into (B,C,T)\n",
        "      B, T, C = logits.shape\n",
        "      # Strech the array to make it 2 dimentional\n",
        "      logits = logits.view(B*T, C)\n",
        "      # We have to do the same for the target\n",
        "      targets = targets.view(B*T)\n",
        "      # Calculate loss\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"\n",
        "    Generate predictions\n",
        "\n",
        "    Args:\n",
        "      idx: Is (B, T) array of indices in the crr. context\n",
        "      max_new_tokens (int): Max amount of tokens to generate\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # Get the prediction\n",
        "      logits, loss = self(idx)\n",
        "      # We only want to focus on the last time step\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      # Apply soft max to logits -> Get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # Sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # Append sampled index to current sequence\n",
        "      idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "lrcMWmHxrrpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size).to(device)\n",
        "logits, loss = m(xb, yb)\n",
        "\n",
        "print(f\"Example: {logits.shape}\")\n",
        "print(f\"Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBsVnXj7svyA",
        "outputId": "d6f03ab5-e157-4544-92fc-ead3ef2c5855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example: torch.Size([32, 65])\n",
            "Loss: 4.878634929656982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it generated it totally random letters because the model is totally random (no training, nothing). So we would need to train it to start getting some \"real\" text."
      ],
      "metadata": {
        "id": "tfeTx-AVx-nH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "idx = torch.zeros((1, 1), dtype=torch.long, device=device) # Where we kick off the generation\n",
        "generated_list = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
        "\n",
        "print(f\"Example: {decode(generated_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K00ZehfSuQcj",
        "outputId": "ed9c0318-28be-484a-be03-838787d078d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example: \n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's train our Bigram Model"
      ],
      "metadata": {
        "id": "EskxiAgEyg_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AdamW optimizer\n",
        "optimizer = torch.optim.AdamW(params=m.parameters(),\n",
        "                             lr=1e-3)"
      ],
      "metadata": {
        "id": "fHI3MB7Zxu3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop\n",
        "\n",
        "We will train our model for 10.000 epochs."
      ],
      "metadata": {
        "id": "0ANxdI53yxP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 10_000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Get a batch of data\n",
        "  xb, yb = get_batch(split=\"train\")\n",
        "  # 1. Forward pass + Loss calculation\n",
        "  logits, loss = m(xb, yb)\n",
        "  # 2. Optimize zero_grad\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  # 3. Back propagation\n",
        "  loss.backward()\n",
        "  # 4. Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 1000 == 0:\n",
        "    print(f\"Epoch {epoch}: {loss.item()} loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kG2xpF-MytIs",
        "outputId": "2dbc6dd3-0e63-4540-a7e2-9a6b801aa430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 4.692410945892334 loss\n",
            "Epoch 1000: 3.7637593746185303 loss\n",
            "Epoch 2000: 3.2342257499694824 loss\n",
            "Epoch 3000: 2.892245292663574 loss\n",
            "Epoch 4000: 2.703908681869507 loss\n",
            "Epoch 5000: 2.515348196029663 loss\n",
            "Epoch 6000: 2.4889943599700928 loss\n",
            "Epoch 7000: 2.514069080352783 loss\n",
            "Epoch 8000: 2.444497585296631 loss\n",
            "Epoch 9000: 2.3975775241851807 loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could say we had an improvemnt but not much... But at least the loss is going down. Now we need to have better context and let the tokens see what is in the context.\n",
        "\n",
        "Remember that this bigram is an extremely simple model, with the transformer model we will see way better results."
      ],
      "metadata": {
        "id": "szn4nzXc1S8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example with trained model\n",
        "idx = torch.zeros((1, 1), dtype=torch.long, device=device) # Where we kick off the generation\n",
        "generated_list = m.generate(idx, max_new_tokens=100)[0].tolist()\n",
        "\n",
        "print(f\"Example: {decode(generated_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zkoJcbOzZ8B",
        "outputId": "b11ce89d-f417-4821-8a02-814b2ad51e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example: \n",
            "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulsee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note*: We have some insigth in the `matrix_example_attention.ipynb` colab."
      ],
      "metadata": {
        "id": "UcBxSY98I4gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigram Model V2\n",
        "\n",
        "Building forward to our Transformer model"
      ],
      "metadata": {
        "id": "0pvDuYNXKd69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "NUM_EMBEDDINGS = 32\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  \"\"\"\n",
        "  Predicts the next token only using the current token\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size: int):\n",
        "    \"\"\"\n",
        "    Initializes the Bigram class\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, NUM_EMBEDDINGS)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, NUM_EMBEDDINGS)\n",
        "    self.lm_head = nn.Linear(NUM_EMBEDDINGS, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"\n",
        "    Forward method for the BigramLanguageModel\n",
        "    \"\"\"\n",
        "    # (B,T,C) -> (Batch_size, Sequence length, vocab_size)\n",
        "    token_embedding = self.token_embedding_table(idx)\n",
        "    # Positional embedding\n",
        "    pos_embedding = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "\n",
        "    # Hold the token identity + the position where it occures\n",
        "    x = token_embedding + pos_embedding\n",
        "\n",
        "    logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # Transform into (B,C,T)\n",
        "      B, T, C = logits.shape\n",
        "      # Strech the array to make it 2 dimentional\n",
        "      logits = logits.view(B*T, C)\n",
        "      # We have to do the same for the target\n",
        "      targets = targets.view(B*T)\n",
        "      # Calculate loss\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"\n",
        "    Generate predictions\n",
        "\n",
        "    Args:\n",
        "      idx: Is (B, T) array of indices in the crr. context\n",
        "      max_new_tokens (int): Max amount of tokens to generate\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "      # Get the prediction\n",
        "      logits, loss = self(idx_cond)\n",
        "      # We only want to focus on the last time step\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      # Apply soft max to logits -> Get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # Sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # Append sampled index to current sequence\n",
        "      idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "CyHL3Vy5KgL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}