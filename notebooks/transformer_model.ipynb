{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAIRk4uGJrR8"
      },
      "source": [
        "### Transformer Model\n",
        "\n",
        "*Note: This is a character-base transformer.*\n",
        "\n",
        "This is the final version of our transformer model that we built following the `Andrej Karpathy` guide. The idea is to develop all blocks that together they form a transformer block.\n",
        "\n",
        "In addition to this notebook, we have experiment with bigrams and some math insigth (in other notebooks):\n",
        "\n",
        "* Models development: https://colab.research.google.com/drive/13e0Gd4WGlrrcgKF2vbQTY4SG13dwTi3X#scrollTo=oEtJw0HyLU3z\n",
        "\n",
        "* Math insigth: https://colab.research.google.com/drive/10k3GpHnVxSTkPY4jlaQn5IAE3-w8Kndj\n",
        "\n",
        "Paper which this model is based on: https://arxiv.org/abs/1706.03762"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StMx_hy9KSz8"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EHVkgLUNJo80"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOe_VAW3MNH7"
      },
      "source": [
        "### Model Parameters\n",
        "Setup global parameters as global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DTh3SCQwMOd7"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "BATCH_SIZE = 64 # Num of independent sequences to process in parallel\n",
        "BLOCK_SIZE = 256 # Max context for predictions\n",
        "\n",
        "EPOCHS = 2500 # Num of times to run the training loop\n",
        "\n",
        "EVAL_INTERVAL = 250\n",
        "EVAL_ITERS = 200\n",
        "\n",
        "NUM_EMBEDDINGS = 384\n",
        "LEARNING_RATE = 3e-4\n",
        "NUM_HEADS = 6\n",
        "NUM_LAYERS = 6\n",
        "DROPOUT_P = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYgplnuJKU47"
      },
      "source": [
        "### Device Agnostic Code\n",
        "\n",
        "The idea is that our code can run on any device without having to modify the code, so we setup device agnostic code (if we are on cuda it will run there, else in cpu)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TtnSAD2gKnXT",
        "outputId": "0234c3c9-23b2-436e-8233-8a954cd1dd71"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU65tHT2KutM"
      },
      "source": [
        "# Data & Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FuPZvcckRnM"
      },
      "source": [
        "### Download the Shakespeare Dataset\n",
        "\n",
        "1. We are going to download the `.txt` from the karpathy repo.\n",
        "2. We are going to read it and store it in our code variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_WcS0o0ZhQD6",
        "outputId": "26044cd8-3b07-4513-9fce-3c49bb4443ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-12-05 12:48:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-12-05 12:48:23 (30.4 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJaaGVKDkaiw",
        "outputId": "603609c1-a494-4032-e4d2-9da2b5821122"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aprox length of dataset: 1115394\n",
            "\n",
            "First 100 characters of the dataset:\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "with open(file=\"/content/input.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(f\"Aprox length of dataset: {len(text)}\\n\")\n",
        "print(f\"First 100 characters of the dataset:\\n{text[:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7xeFuLTlPnr"
      },
      "source": [
        "### Build vocabulary based on characters (Character-level)\n",
        "\n",
        "This will be the characters that the model can see or 'generate'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcWy53Hxkzoh",
        "outputId": "96cd4ccf-0f0b-40b1-962c-46dbaa66d3da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "Vocab size: 65\n"
          ]
        }
      ],
      "source": [
        "# Characters\n",
        "characters = sorted(list(set(text)))\n",
        "# Vocabulary size\n",
        "vocab_size = len(characters)\n",
        "\n",
        "print(f\"Characters: {''.join(characters)}\")\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGS25mNWl_tb"
      },
      "source": [
        "### Character level lookup tables\n",
        "\n",
        "This is a simple tokenizer for our characters/vocab. We will have a loop up table to transform characters into tokens/ints and another to do the reverse process (transform tokens/ints to characters/text)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qQYAtduClk2Y"
      },
      "outputs": [],
      "source": [
        "# We create the lookup tables (Character-base)\n",
        "string_to_id = { char:idx for idx, char in enumerate(characters)}\n",
        "id_to_string = { idx:char for idx, char in enumerate(characters)}\n",
        "\n",
        "# Create the functions to look words in the lookup tables\n",
        "encode = lambda sentence: [string_to_id[char] for char in sentence]\n",
        "decode = lambda id_list: \"\".join(id_to_string[id] for id in id_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQhRxWb4nfcA",
        "outputId": "9e19fcad-d6e2-4a35-985c-f04256972ce3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# Testing our our tokenizers\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tHpD_-Jns6r"
      },
      "source": [
        "### Tokenize the entire text\n",
        "\n",
        "We are going to encode the whole dataset and transform it into a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZCB0ykxnk0A",
        "outputId": "4214f1e1-3ce8-4b02-a46c-b778785e468a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data tensor shape: (torch.Size([1115394]), torch.int64)\n",
            "First 100 encodes: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "print(f\"Data tensor shape: {data.shape, data.dtype}\")\n",
        "print(f\"First 100 encodes: {data[:100]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Aciycd2oRBc"
      },
      "source": [
        "### Split data between training & validation set\n",
        "\n",
        "This will allow us to train our model and later test it with data it hasn't seen yet. We are going to do it manually (90% for training)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75xoHhzuoJNJ",
        "outputId": "b6e7a452-e06c-424b-a8e1-3294602ede36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training length: 1003854\n",
            "Validation length: 111540\n"
          ]
        }
      ],
      "source": [
        "# Get the index where we need to \"cut\"\n",
        "n = int(0.9 * len(data))\n",
        "# Training set\n",
        "train_data = data[:n]\n",
        "# Validation set\n",
        "val_data = data[n:]\n",
        "\n",
        "print(f\"Training length: {len(train_data)}\")\n",
        "print(f\"Validation length: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bkx7YYWo9zL"
      },
      "source": [
        "### Context size (Block size)\n",
        "\n",
        "We want to sample parts/chunks of the text. The idea is that the model can see want should come next from one character up to block size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "I1Uw18lGpkng"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "def get_batch(split: Literal[\"train\", \"validation\"]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "  \"\"\"\n",
        "  Generate a small batch of data of inputs x and targets y.\n",
        "  It first generates 4 random locations in the dataset, and then extract the data for each of those indexes.\n",
        "\n",
        "  Args:\n",
        "    split (\"train\" | \"validation\"): What data split to use\n",
        "  Returns:\n",
        "    x -> Tensor\n",
        "    y -> Tensor\n",
        "  \"\"\"\n",
        "  data = train_data if split == \"train\" else val_data\n",
        "  # Sample random parts\n",
        "  ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "  # Get the actual data for that random sample\n",
        "  x = torch.stack([data[i:i+BLOCK_SIZE] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+BLOCK_SIZE+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YC2aUD4urosR",
        "outputId": "521b7870-ec87-4edc-c78a-df7d3e5eeee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([64, 256])\n",
            "tensor([[ 1, 52, 43,  ..., 63,  1, 57],\n",
            "        [ 1, 55, 59,  ..., 28, 53, 51],\n",
            "        [15, 39, 54,  ..., 52,  1, 21],\n",
            "        ...,\n",
            "        [40, 59, 58,  ..., 43, 52,  1],\n",
            "        [ 1, 41, 53,  ..., 63,  6,  0],\n",
            "        [59, 58,  1,  ..., 58, 47, 53]], device='cuda:0')\n",
            "targets:\n",
            "torch.Size([64, 256])\n",
            "tensor([[52, 43, 60,  ...,  1, 57, 54],\n",
            "        [55, 59, 53,  ..., 53, 51, 44],\n",
            "        [39, 54, 43,  ...,  1, 21,  1],\n",
            "        ...,\n",
            "        [59, 58,  6,  ..., 52,  1, 50],\n",
            "        [41, 53, 61,  ...,  6,  0, 14],\n",
            "        [58,  1, 58,  ..., 47, 53,  2]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj1IJSLQKaSz"
      },
      "source": [
        "# Transformer Architecture\n",
        "\n",
        "After processing all the data, in this section we will build the transformer blocks:\n",
        "\n",
        "* Attention Head\n",
        "* Multi-Head Attention\n",
        "* FeedForward (FF block)\n",
        "* Layer Norm\n",
        "* Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CoeQi9TLJ2c"
      },
      "source": [
        "### Attention (Head & Multi-Head)\n",
        "\n",
        "This section focuses on implementing the core attention mechanisms. It defines the `Head` class, which represents a single self-attention head, responsible for computing queries, keys, and values, and then calculating attention scores. It also includes the `MultiHeadAttention` class, which combines several Head instances to allow the model to attend to different parts of the input simultaneously, enhancing its ability to capture diverse relationships within the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZBlZiilGK601"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\"\n",
        "  Self-Attention head\n",
        "  \"\"\"\n",
        "  def __init__(self, head_size: int):\n",
        "    \"\"\"\n",
        "    Initializes the head class\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Represents \"what I contain\" — the features others can look at\n",
        "    self.key = nn.Linear(in_features=NUM_EMBEDDINGS,\n",
        "                         out_features=head_size,\n",
        "                         bias=False)\n",
        "    # Represents \"what I'm looking for\" — used to score compatibility with keys\n",
        "    self.query = nn.Linear(in_features=NUM_EMBEDDINGS,\n",
        "                         out_features=head_size,\n",
        "                         bias=False)\n",
        "    # Represents \"what I will send\" — the information that gets aggregated\n",
        "    self.value = nn.Linear(in_features=NUM_EMBEDDINGS,\n",
        "                         out_features=head_size,\n",
        "                         bias=False)\n",
        "    self.register_buffer(\"tril\", torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "    # Dropout layer\n",
        "    self.dropout = nn.Dropout(p=DROPOUT_P)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass for Head model\n",
        "    \"\"\"\n",
        "    # Batch, Time, Channels\n",
        "    B, T, C = x.shape\n",
        "    # Create key and query\n",
        "    k = self.key(x) # (B, T, head_size)\n",
        "    q = self.query(x) # (B, T, hs)\n",
        "\n",
        "\n",
        "    # Multiplication of keys with queries (To compute attention scores (\"affinities\"))\n",
        "    wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    # Perform the weighted aggregation of the values\n",
        "    v = self.value(x) # (B, T, hs)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2nMFPGXCMBHM"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  \"\"\"\n",
        "  Multi-Head Attention class\n",
        "  \"\"\"\n",
        "  def __init__(self, num_heads: int, head_size: int):\n",
        "    \"\"\"\n",
        "    Initializes MultiHeadAttention class\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.projection = nn.Linear(in_features=NUM_EMBEDDINGS,\n",
        "                                out_features=NUM_EMBEDDINGS)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass for Multi-Head attention\n",
        "    \"\"\"\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    return self.projection(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyYtB5tkO0w8"
      },
      "source": [
        "### Feed Forward (FF Block)\n",
        "\n",
        "This layer will happend after Self-Attention (each time one SA appears, this one goes next). In basic words, it allows to process what we sort of figured out in the attention block and don't go TOO fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3JIgdRuVO33F"
      },
      "outputs": [],
      "source": [
        "class FeedFoward(nn.Module):\n",
        "  \"\"\"\n",
        "  Simple linear layer followed by a non-linearity\n",
        "  \"\"\"\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        # Multiply by 4 following the paper (to grow the layer)\n",
        "        nn.Linear(in_features=n_embd, out_features=4*n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(in_features=4*n_embd, out_features=n_embd),\n",
        "        nn.Dropout(p=DROPOUT_P),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass for FeedForward\n",
        "    \"\"\"\n",
        "    return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utGOTHcdPys0"
      },
      "source": [
        "### Transformer Block\n",
        "\n",
        "Transformer block without the Encoder-Decoder Attention (We are skipping that for now, as this is a GPT like decoder model)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DmxhVIk5P7qN"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer block module\n",
        "  \"\"\"\n",
        "  def __init__(self, num_embedding: int, n_head: int):\n",
        "    \"\"\"\n",
        "    Initializes the Block class\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    head_size = num_embedding // n_head\n",
        "    self.self_attention = MultiHeadAttention(num_heads=n_head,\n",
        "                                             head_size=head_size)\n",
        "    self.ff = FeedFoward(num_embedding)\n",
        "    # Layer norms\n",
        "    self.ln1 = nn.LayerNorm(num_embedding)\n",
        "    self.ln2 = nn.LayerNorm(num_embedding)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Forward pass for Block module\n",
        "    \"\"\"\n",
        "    # With residual/skip connection\n",
        "    x = x + self.self_attention(self.ln1(x))\n",
        "    x = x + self.ff(self.ln2(x))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNf8izLDVJOk"
      },
      "source": [
        "### Model Class\n",
        "\n",
        "This section defines the main `LanguageModel` class, which brings together all the previously defined components to form the complete transformer decoder model (similar to a GPT architecture). It includes token and positional embeddings, a sequence of transformer `Blocks` (which encapsulate multi-head attention and feed-forward layers), a final layer normalization, and a linear head for outputting the vocabulary logits. This class also contains the forward method for training and a generate method for sampling new text from the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "plegbyvOQMlu"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\"\n",
        "  Shakespeare Language Model (Decoder)\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initializes the LanguageModel module\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    # Each token directly reads off the logits for the next token from a lookup table\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, NUM_EMBEDDINGS)\n",
        "    self.position_embedding_table = nn.Embedding(BLOCK_SIZE, NUM_EMBEDDINGS)\n",
        "    # Create the blocks\n",
        "    self.blocks = nn.Sequential(*[Block(num_embedding=NUM_EMBEDDINGS,\n",
        "                                        n_head=NUM_HEADS) for _ in range(NUM_LAYERS)])\n",
        "    # Add extra layer norm\n",
        "    self.layer_norm = nn.LayerNorm(NUM_EMBEDDINGS) # Final layer norm\n",
        "    # Layer that outputs the vocab_size\n",
        "    self.lm_head = nn.Linear(in_features=NUM_EMBEDDINGS,\n",
        "                             out_features=vocab_size)\n",
        "    # Weights initialization\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    \"\"\"\n",
        "    Performs the forward pass of the LanguageModel\n",
        "    \"\"\"\n",
        "    B, T = idx.shape\n",
        "\n",
        "    # (B,T,C) -> (Batch_size, Sequence length, vocab_size)\n",
        "    token_embedding = self.token_embedding_table(idx)\n",
        "    # Positional embedding\n",
        "    pos_embedding = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
        "\n",
        "    # Hold the token identity + the position where it occures\n",
        "    x = token_embedding + pos_embedding # (B, T, C)\n",
        "\n",
        "    # Process x\n",
        "    x = self.blocks(x) # (B, T, C)\n",
        "    x = self.layer_norm(x) # (B, T, C)\n",
        "    logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # Transform into (B,C,T)\n",
        "      B, T, C = logits.shape\n",
        "      # Strech the array to make it 2 dimentional\n",
        "      logits = logits.view(B*T, C)\n",
        "      # We have to do the same for the target\n",
        "      targets = targets.view(B*T)\n",
        "      # Calculate loss\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    \"\"\"\n",
        "    Generate predictions\n",
        "\n",
        "    Args:\n",
        "      idx: Is (B, T) array of indices in the crr. context\n",
        "      max_new_tokens (int): Max amount of tokens to generate\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "      # crop idx to the last block_size tokens\n",
        "      idx_cond = idx[:, -BLOCK_SIZE:]\n",
        "      # Get the prediction\n",
        "      logits, loss = self(idx_cond)\n",
        "      # We only want to focus on the last time step\n",
        "      logits = logits[:, -1, :] # (B, C)\n",
        "      # Apply soft max to logits -> Get probabilities\n",
        "      probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "      # Sample from the distribution\n",
        "      idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "      # Append sampled index to current sequence\n",
        "      idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B09QODwZBcE"
      },
      "source": [
        "# Training & Evaluating the Model\n",
        "\n",
        "We will be using Google Colab with a T4 GPU to train this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p620vZXWZwKE"
      },
      "source": [
        "### Model Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C2xKbuKZZJFG",
        "outputId": "72d4d745-080f-4c94-96e0-5489afe26fef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (token_embedding_table): Embedding(65, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (ff): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (ff): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (ff): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (ff): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (ff): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (self_attention): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (projection): Linear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (ff): FeedFoward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LanguageModel().to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XwuntYcaXK-",
        "outputId": "b6d236f5-77e5-4c58-b3f1-8d5e04842a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Language Model parameters count: 10.79M\n"
          ]
        }
      ],
      "source": [
        "model_parameters_count = sum(p.numel() for p in model.parameters())/1e6\n",
        "print(f\"Language Model parameters count: {model_parameters_count:.2f}M\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEocOl1ObYTs"
      },
      "source": [
        "### Optimizer\n",
        "\n",
        "Initialize the AdamW optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "L5i7x8hrbEme"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvX_rq5jcQFF"
      },
      "source": [
        "### Helper function to evaluate the loss\n",
        "\n",
        "* Turns off gradients\n",
        "* Evaluates model in eval() mode\n",
        "* Runs several mini-batches from both train and val\n",
        "* Averages their loss to reduce noise\n",
        "* Puts model back into training mode\n",
        "* Returns train & val loss estimates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cqpgGhhscSDM"
      },
      "outputs": [],
      "source": [
        "# Decorator to disable gradient tracking\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  \"\"\"\n",
        "  Computes an average loss for train and validation sets without updating the model.\n",
        "  \"\"\"\n",
        "  # Output dict containing \"train\" and \"val\"\n",
        "  out = {}\n",
        "  # Set model into evaluation mode\n",
        "  model.eval()\n",
        "  # Looping for both datasets\n",
        "  for split in [\"train\", \"val\"]:\n",
        "    losses = torch.zeros(EVAL_ITERS)\n",
        "\n",
        "    for k in range(EVAL_ITERS):\n",
        "      X, Y = get_batch(split)\n",
        "      logits, loss = model(X, Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "\n",
        "  # Set the model back to training before returning out\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk1R8Bx-bkoM"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "We will train our model using the training loop, folowwing this steps:\n",
        "\n",
        "1. Fordward propagation.\n",
        "2. Calculate the loss.\n",
        "3. Optimize `zero_grad`.\n",
        "4. Back propagation.\n",
        "5. Optimization step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXU_XSUFbrnu",
        "outputId": "46de19d1-8114-4261-e853-cd41c2bff34c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: train loss 4.2325, val loss 4.2290\n",
            "Step 250: train loss 2.3099, val loss 2.3488\n",
            "Step 500: train loss 1.7006, val loss 1.8510\n",
            "Step 750: train loss 1.4513, val loss 1.6621\n",
            "Step 1000: train loss 1.3312, val loss 1.5614\n",
            "Step 1250: train loss 1.2588, val loss 1.5283\n",
            "Step 1500: train loss 1.2043, val loss 1.5166\n",
            "Step 1750: train loss 1.1471, val loss 1.5071\n",
            "Step 2000: train loss 1.1006, val loss 1.4966\n",
            "Step 2250: train loss 1.0568, val loss 1.5104\n",
            "Step 2499: train loss 1.0135, val loss 1.5259\n"
          ]
        }
      ],
      "source": [
        "# Training Loop for EPOCHS amount of times\n",
        "for epoch in range(EPOCHS):\n",
        "  # Every once in a while evaluate the loss on train and val sets\n",
        "  if epoch % EVAL_INTERVAL == 0 or epoch == EPOCHS - 1:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"Step {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  # Get a batch of data\n",
        "  xb, yb = get_batch(split=\"train\")\n",
        "  # 1. Forward pass + Loss calculation\n",
        "  logits, loss = model(xb, yb)\n",
        "  # 2. Optimize zero_grad\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  # 3. Back propagation\n",
        "  loss.backward()\n",
        "  # 4. Optimizer step\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwIxK6e9mRZ1"
      },
      "source": [
        "*Note: Maybe the amount of epochs were a bit too much or idk, but the train loss decreased very well but the val started to go up a bit and not always down. This means it's making some mistakes when predicting...\n",
        "Should we train for less or how could we improve this?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amxjOXTbgsAO"
      },
      "source": [
        "### Making Predictions with our Model\n",
        "\n",
        "We are going to generate a sequence of 500 tokens to see how well our model is generating the Shakespeare text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GjzZXhYgrvA",
        "outputId": "f9e4cf30-d515-4664-e675-9c66d1a931cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "KING RICHARD II:\n",
            "What, is my frescrophect?\n",
            "\n",
            "KING RICHARD III:\n",
            "Norfolk?\n",
            "\n",
            "ENORTHUMBERLAND:\n",
            "Yea, say you will tay amend.\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "You do him deliver\n",
            "That thought have your daughter's womb!\n",
            "It wasted for this disdague hell hence to the will,\n",
            "For my husband wait your hight skill such is always.\n",
            "What is't the nothing boy?\n",
            "\n",
            "CAMILLO:\n",
            "Old comes my stearves.\n",
            "\n",
            "LEONTES:\n",
            "You are upon that in my colderatal tale!\n",
            "\n",
            "CAMILLO:\n",
            "See my lord,\n",
            "Lady did not your honour law, that they vay more\n",
            "Be in my sword: 'v\n"
          ]
        }
      ],
      "source": [
        "# Generate from our LnaguageModel model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "# Generating & printing 500 generated tokens\n",
        "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvKRs31HgZGl"
      },
      "source": [
        "### Save Model Locally\n",
        "\n",
        "We are going to save our LanguageModel so we can use it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "g_QPFKeodzIf"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/content/transformer_model.pth\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
